---
keywords:
  - deep learning
  - neural network
  - AI
  - Artificial Intelligent
  - 人工智慧
title: 用Alpha Zero完成期末作業結論
comments: true
---

反正故事大概是這樣：交大人工智慧概論期末是一個人工智慧遊戲競賽，比的是一種從來沒有玩過的遊戲。為了贏得比賽我跟隊友直接訓練 Alpha Zero 類神經網路，然而中途殺出新規則限制硬體規格與不能使用套件（numpy 除外）的規定，讓我們中途放棄改用魔改 蒙地卡羅樹狀搜尋（MCTS）來繳交期末作業。

遊戲規則大致如下：

遊戲採用 8x8 的西洋棋格局下，雙方的黑、白棋子在終局抵達對方的陣營最多者勝利。每個棋子只能上下左右走一步。在特殊情況下，棋子的上、下、左、右被一顆棋子當著，棋子可以直接跳過去（前提是落腳的點沒有任何棋子擋著）。如果被跳過的棋子是敵方棋子，你可以“吃掉”那一個棋子。如此一來敵方能抵達你陣營的棋子就減少了，同時也減少敵方棋子在前行時會吃掉自己棋子的機率。

規則就是這麼簡單。

劇透：因為利用自己實作的 numpy net 結果與實際 Pytorch 的結果不一樣，我們最後放棄使用 Alpha Zero 方法。而直接用微改的 reward function + 人工的 policy function 來完成遊戲 AI


## Alpha Zero

Alpha Go Zero 為Deepmind 圍棋AI版本 2, 而 Alpha Zero 為 Deepmind 針對完美資訊遊戲的終極版本。比起第 2 版本更加精簡（去除了evaluation 階段），訓練過程的主要的元件可以大略分為兩個： 蒙地卡羅樹狀搜尋 (MCTS)、自我學習。

如果使用類神經網路，我們會用棋盤的資訊作為輸入，並輸出接下來的下一步可以走什麼。為了增加穩定度，deepmind 加了一個  value network 預測目前棋盤的勝率（-1 輸～0 平手～1 勝）。那要如何在沒有任何前提資訊下，讓這個類神經網路學習？很簡單，模擬環境，再讓他與一位比他更好的對象學習。

學習的重點在於從比你更好的老師學習，這個老師在 Alpha Go Zero 定義為與學生對局有55%的勝率。然而在 Alpha Zero 這個老師為使用 MCTS 搜尋過的。某程度上MCTS + 類神經網路會比起類神經網路還更好，因為 MCTS 讓類神經網路“探索”了未來的可能。這機制讓 self play 過程更加穩定，如果去掉MCTS 目前沒有說明這是可行的（如果有應該也很難復刻）。


## 魔改MCTS版本

由於時間緊湊，我們無法寫出一個高效能的 MCTS 演算法，因此每一局自我對局需要 15～20分鐘才能完成。如果依照論文中每一局結束後就針對類神經網路做訓練，速度會非常的慢。因此我們讓模型同時進行 13 場自我對局，結束後再做訓練。

此外，每一局展開蒙地卡羅樹節點（論文中稱為 playout ）需要耗費大量的時間，因此我們能限制在130 步而論文中的 playout 為上千次。因此我估測在這麼小的 playout 選擇的步非常多噪音，如此小的蒙地卡羅樹可參考的資訊非常有限。因此模型的損失值一直無法有效的收斂。 

### 學長：我們模型收斂不了怎麼辦？

因為這遊戲規則有吃掉敵方棋子的規則，於是我認為只要採用貪心計策，在展開 MCTS 節點時將 playout 過程中的吃到的棋子也考慮進獎勵值中，使 AI 在選擇能吃到對方最多棋子的方向邁進。而這計策唯一的漏洞就是可能造成太早吃完對方棋子，自己棋子在敵方陣營太少，造成提升平局出現機率。

因此我嘗試將訓練了20000次的類神經模型搭配自己的貪心計策（稱為 Jump Zero + Greedy policy）與原始MCTS+貪心計策互相對弈。在20場輪流替換顏色的比賽下來， Jump Zero+ Greedy Policy 可以獲得15場的勝利。

如果你喜歡我的文章或side projects，可以捐贈我一杯[大熱美（大杯美式咖啡）](https://www.buymeacoffee.com/theblackcat102)支持我。