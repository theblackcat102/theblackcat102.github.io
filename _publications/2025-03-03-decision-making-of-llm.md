---
title: "Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models"
collection: publications
permalink: /publication/2025-03-03-decision-making-of-llm
excerpt: 'This study formalizes the task of risk-aware decision making in LLMs, explores how models adapt their decisions to different risk levels, and proposes skill decomposition solutions to improve performance. The findings show that even advanced LMs require explicit prompt chaining to handle risk-aware decision making effectively.'
date: 2025-03-03
category: conferences
venue: 'arXiv'
paperurl: 'https://arxiv.org/abs/2503.01332'
citation: 'Wu, C.K., Tam, Z.R., Lin, C.Y., Chen, Y.N., &amp; Lee, H. (2024). &quot;Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models.&quot; arXiv preprint arXiv:2503.01332.'
---

<a href='https://arxiv.org/abs/2503.01332'>Download paper here</a>

This study formalizes the task of risk-aware decision making in language models, examining how models should adapt their decisions (answer or refuse) based on contextual risk levels. The researchers identify key failure modes where LMs make irrational decisions in both high-risk and low-risk scenarios and propose skill decomposition solutions that separate downstream task performance, confidence estimation, and expected-value reasoning. Results show that prompt chaining significantly improves performance in high-risk settings across multiple models and datasets.

Recommended citation: Wu, C.K., Tam, Z.R., Lin, C.Y., Chen, Y.N., & Lee, H. (2024). "Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models." arXiv preprint arXiv:2503.01332.