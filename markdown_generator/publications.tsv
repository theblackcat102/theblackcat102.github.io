pub_date	title	venue	excerpt	citation	url_slug	paper_url	slides_url
2024-03-04	An improved traditional chinese evaluation suite for foundation model	arXiv	We present TMMLU+, a new benchmark designed for Traditional Chinese language understanding. TMMLU+ is a multi-choice question-answering dataset with 66 subjects from elementary to professional level. It is six times larger and boasts a more balanced subject distribution than its predecessor, Taiwan Massive Multitask Language Understanding (TMMLU).	Tam, Z.R., Pai, Y.T., Lee, Y.W., Chen, J.D., Chu, W.M., Cheng, S., & Shuai, H.H. (2024). "An improved traditional chinese evaluation suite for foundation model." arXiv preprint arXiv:2403.01858.	improved-traditional-chinese-evaluation	https://arxiv.org/abs/2403.01858	
2024-04-25	Personalized EDM Subject Generation via Co-factored User-Subject Embedding	PAKDD	This paper introduces the Co-Factored User-Subject Embedding based Personalized EDM Subject Generation Framework (COUPES), a model for creating personalized Electronic Direct Mail (EDM) subjects.	Chen, Y.H., Tam, Z.R., & Shuai, H.H. (2024). "Personalized EDM Subject Generation via Co-factored User-Subject Embedding." Pacific-Asia Conference on Knowledge Discovery and Data Mining, 55-67.	personalized-edm-subject-generation	https://link.springer.com/chapter/10.1007/978-981-97-2253-2_5	
2022-05-10	Improving entity disambiguation using knowledge graph regularization	PAKDD	Entity disambiguation plays the role on bridging between words of interest from an input text document and unique entities in a target Knowledge Base (KB).	Tam, Z.R., Wu, Y.L., & Shuai, H.H. (2022). "Improving entity disambiguation using knowledge graph regularization." Pacific-Asia Conference on Knowledge Discovery and Data Mining, 341-353.	entity-disambiguation-kg	https://link.springer.com/chapter/10.1007/978-3-031-05933-9_27	
2024-07-20	I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation	arXiv	This study explores the proactive ability of LLMs to seek user support. We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability.	Wu, C.K., Tam, Z.R., Wu, C.C., Lin, C.Y., Lee, H., & Chen, Y.N. (2024). "I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation." arXiv preprint arXiv:2407.14767.	i-need-help-llm	https://arxiv.org/abs/2407.14767	
2024-11-01	Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance	EMNLP Industry Track	Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).	Tam, Z.R., Wu, C.K., Tsai, Y.L., Lin, C.Y., Lee, H., & Chen, Y.N. (2024). "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance." EMNLP Industry Track, 1218-1236.	let-me-speak-freely	https://aclanthology.org/2024.emnlp-industry.91	
2024-02-13	Openassistant conversations-democratizing large language model alignment	NeurIPS	Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT.	Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.R., et al. (2024). "Openassistant conversations-democratizing large language model alignment." NeurIPS, 36.	openassistant-conversations	https://proceedings.neurips.cc/paper_files/paper/2023/hash/949f0f8f32267d297c2d4e3ee10a2e7e-Abstract-Datasets_and_Benchmarks.html	
2020-08-23	Character-preserving coherent story visualization	ECCV	Story visualization aims at generating a sequence of images to narrate each sentence in a multi-sentence story.	Song, Y.Z., Tam, Z.R., Chen, H.J., Lu, H.H., & Shuai, H.H. (2020). "Character-preserving coherent story visualization." European Conference on Computer Vision, 18-33.	character-preserving-story	https://link.springer.com/chapter/10.1007/978-3-030-58520-4_2	
2021-10-01	Gradient normalization for generative adversarial networks	ICCV	In this paper, we propose a novel normalization method called gradient normalization (GN) to tackle the training instability of Generative Adversarial Networks (GANs) caused by the sharp gradient space.	Wu, Y.L., Shuai, H.H., Tam, Z.R., & Chiu, H.Y. (2021). "Gradient normalization for generative adversarial networks." ICCV, 6373-6382.	gradient-normalization-gan	http://openaccess.thecvf.com/content/ICCV2021/html/Wu_Gradient_Normalization_for_Generative_Adversarial_Networks_ICCV_2021_paper.html